{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca4783c-88e0-411e-a169-d2eb4b0ad2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from utils import seed_everything, batch_plot\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97fdaad-92ac-4791-8646-a8273834bfe5",
   "metadata": {},
   "source": [
    "# Sinusoidal Positional Encoding\n",
    "\n",
    "advantages:\n",
    "\n",
    "1. Provide unique encoding for each position in the sequence.\n",
    "2. Each adjacent position have the same relative distance.\n",
    "3. The encoding can be extended to arbitrary length of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e203bf7-122c-44a3-bf17-1854ea609ac9",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "\\sin\\left(i \\cdot 10000^{-\\frac{j}{d}}\\right) & \\text{if j is even} \\\\\n",
    "\\cos\\left(i \\cdot 10000^{-\\frac{(j-1)}{d}}\\right) & \\text{otherwise} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_t =\n",
    "\\begin{bmatrix}\n",
    "\\sin(\\omega_1t) & \\cos(\\omega_1t) & \\sin(\\omega_2t) & \\cos(\\omega_2t) & \\cdots & \\sin(\\omega_{d/2}t) & \\cos(\\omega_{d/2}t)\n",
    "\\end{bmatrix}_{1 \\times d}\n",
    "$$\n",
    "\n",
    "where $\\omega_n = 10000^{-\\frac{n}{d}}$ and $d$ is the embedding size.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Vert PE_{t+1} - PE_t \\Vert &= \\sqrt{\\sum_{n=1}^{d/2} \\Bigl(\\bigl( \\sin(\\omega_nt+\\omega_n) - \\sin(\\omega_nt) \\bigr)^2 + \\bigl(\\cos(\\omega_nt+\\omega_n) - \\cos(\\omega_nt) \\bigr)^2\\Bigr)} \\\\\n",
    "&= \\sqrt{\\sum_{n=1}^{d/2} \\left( \\sin^2(\\omega_nt+\\omega_n) - 2\\sin(\\omega_nt+\\omega_n)\\sin(\\omega_nt) + \\sin^2(\\omega_nt) + \\cos^2(\\omega_nt+\\omega_n) - 2\\cos(\\omega_nt+\\omega_n)\\cos(\\omega_nt) + \\cos^2(\\omega_nt) \\right)} \\\\\n",
    "&= \\sqrt{\\sum_{n=1}^{d/2} \\Bigl( 2 - 2\\bigl(\\cos(\\omega_nt+\\omega_n)\\cos(\\omega_nt) + \\sin(\\omega_nt+\\omega_n)\\sin(\\omega_nt)\\bigr) \\Bigr)} \\\\\n",
    "&= \\sqrt{\\sum_{n=1}^{d/2} \\bigl( 2 - 2\\cos(\\omega_n) \\bigr)} \\\\\n",
    "&= \\sqrt{d - 2\\sum_{n=1}^{d/2}\\cos(\\omega_n)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_positional_encoding(max_length, embedding_size):\n",
    "    positional_encoding = np.empty((max_length, embedding_size))\n",
    "    positions = np.arange(max_length)[:, None]\n",
    "    frequencies = 10000 ** (-np.arange(0, embedding_size, 2) / embedding_size)\n",
    "    positional_encoding[:, 0::2] = np.sin(positions * frequencies)\n",
    "    positional_encoding[:, 1::2] = np.cos(positions * frequencies)\n",
    "    return positional_encoding\n",
    "\n",
    "\n",
    "def generate_binary_encoding(max_length):\n",
    "    encoding = np.empty((max_length, 64), dtype=np.uint8)\n",
    "    for i in range(max_length):\n",
    "        encoding[i] = np.asarray(list(reversed(np.binary_repr(i, width=64))), dtype=np.uint8)\n",
    "    return encoding\n",
    "\n",
    "\n",
    "display(pd.DataFrame(generate_binary_encoding(4)[:, :8], columns=[f\"B_{i}\" for i in range(8)]))\n",
    "pd.DataFrame(generate_positional_encoding(4, 8), columns=[f\"P_{i}\" for i in range(8)])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c15929452c4cdba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d534ed-27a8-4e14-912e-4eebb164bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 500\n",
    "max_length = 1000\n",
    "\n",
    "positional_encoding = generate_positional_encoding(max_length, embedding_size)\n",
    "relative_distance = np.sqrt(embedding_size - 2 * np.cos(frequencies).sum())\n",
    "\n",
    "assert np.allclose(\n",
    "    np.linalg.norm(np.diff(positional_encoding, axis=0), axis=1), relative_distance\n",
    "), \"relative distances should be equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "plt.imshow(generate_binary_encoding(max_length).T, cmap=\"gray_r\")\n",
    "plt.xlabel(\"position\")\n",
    "plt.ylabel(\"bit\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc47aa2cf0181a17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd276b9-4961-45f4-9114-a50c088b4ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose for better visualization\n",
    "transposed_encoding = positional_encoding.T  # (embedding_size, max_length)\n",
    "\n",
    "view_dims = 64\n",
    "\n",
    "_, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12), gridspec_kw={\"height_ratios\": [1, 2]})\n",
    "ax1.imshow(transposed_encoding, cmap=\"gray_r\")\n",
    "ax2.imshow(transposed_encoding[:view_dims, :view_dims], cmap=\"gray_r\")\n",
    "ax2.set_xlabel(\"position\")\n",
    "ax2.set_ylabel(\"embedding dimension\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for position in range(8):\n",
    "    batch_plot(transposed_encoding[:view_dims, position], flatten_layout=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8df3032520a573b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (max_length, 1, embedding_size) - (1, max_length, embedding_size) -> (max_length, max_length, embedding_size)\n",
    "distances = np.linalg.norm(\n",
    "    positional_encoding[:, None] - positional_encoding[None, :], axis=2\n",
    ")  # (max_length, max_length)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(distance[:view_dims, :view_dims], cmap=\"gray_r\")\n",
    "plt.xlabel(\"position\")\n",
    "plt.ylabel(\"position\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c6e8f7bc0094fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c8290-8d72-4f9d-b41f-13a3f316becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_depth = 4\n",
    "plot_step = 4\n",
    "plot_nums = 32\n",
    "\n",
    "\n",
    "_, axs = plt.subplots(nrows=4, ncols=2, figsize=(12, 16), sharex=True, sharey=True)\n",
    "for i, (axo, axe) in enumerate(axs, 1):\n",
    "    even_index, odd_index = 2 * i * plot_step, 2 * i * plot_step - 1\n",
    "    axo.plot(positional_encoding[:plot_nums, odd_index], label=odd_index)\n",
    "    axo.grid()\n",
    "    axo.legend(loc=\"upper right\")\n",
    "    axe.plot(positional_encoding[:plot_nums, even_index], label=even_index)\n",
    "    axe.grid()\n",
    "    axe.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8085596d-aee0-471c-9300-81a160bf5cdd",
   "metadata": {},
   "source": [
    "# references\n",
    "\n",
    "- [Linear Relationships in the Transformerâ€™s Positional Encoding](https://blog.timodenk.com/linear-relationships-in-the-transformers-positional-encoding/)\n",
    "- [Tutorial 6: Transformers and Multi-Head Attention](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- [Master Positional Encoding: Part I](https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)\n",
    "- [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
    "- [Why multi-head self attention works: math, intuitions and 10+1 hidden insights](https://theaisummer.com/self-attention/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
