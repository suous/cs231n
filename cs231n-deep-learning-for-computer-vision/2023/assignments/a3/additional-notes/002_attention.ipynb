{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca4783c-88e0-411e-a169-d2eb4b0ad2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from utils import seed_everything, heatmap\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd89ac0-a3dd-42c4-a113-27844ab5a12e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{attn}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left( \\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} \\right)\\mathbf{V}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a5674-4232-4507-9bd8-1db2a933b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_cp = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    name=\"github\",\n",
    "    colors=[\n",
    "        \"#ebedf0\",\n",
    "        \"#9be9a8\",\n",
    "        \"#40c463\",\n",
    "        \"#30a14e\",\n",
    "        \"#216e39\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def single_head_self_attention(q, k, v, with_scale=True, draw=True, sentences=None):\n",
    "    if sentences is None:\n",
    "        sentences = []\n",
    "\n",
    "    batch_size, value_sentence_size, embedding_size = v.shape\n",
    "    scale = np.sqrt(embedding_size) if with_scale else 1\n",
    "\n",
    "    corr = softmax(q @ k.transpose((0, 2, 1)) / scale)\n",
    "    assert corr.shape == (batch_size, value_sentence_size, value_sentence_size)\n",
    "\n",
    "    if draw:\n",
    "        for i, sentence in zip(range(batch_size), sentences):\n",
    "            sentence = sentence.split()\n",
    "            corr_df = pd.DataFrame(corr[i], columns=sentence, index=sentence)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(16, 16))\n",
    "            heatmap(corr_df, sentence, sentence, ax=ax, cmap=github_cp)\n",
    "            plt.show()\n",
    "    return corr @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e058303-df40-4b49-a3f2-80b5cdd55e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I love this movie\", \"This movie is bad\"]\n",
    "\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "with open(\"sample_words.json\", \"r\") as f:\n",
    "    sample_words = json.load(f)\n",
    "\n",
    "x = np.asarray([[sample_words[w] for w in sentence.lower().split()] for sentence in sentences])  # (N, T, E)\n",
    "batch_size, _, embedding_size = x.shape\n",
    "\n",
    "# for better illustration\n",
    "wq = np.eye(embedding_size)\n",
    "wk = np.eye(embedding_size)\n",
    "wv = np.eye(embedding_size)\n",
    "\n",
    "q = x.copy() @ wq\n",
    "k = x.copy() @ wk\n",
    "v = x.copy() @ wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7edce1-32e4-4e5d-81ac-035da22494e9",
   "metadata": {},
   "source": [
    "# Single head\n",
    "\n",
    "## without scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c41cd9d-1e59-4227-95a1-546f955f7dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = single_head_self_attention(q, k, v, with_scale=False, sentences=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef359684-5e59-44d8-b818-ef32dfa931ac",
   "metadata": {},
   "source": [
    "## with scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63011b3f-57a3-4ab9-8c7f-72e9cf06b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = single_head_self_attention(q, k, v, with_scale=True, sentences=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dacce8-61d1-4b9d-b212-bd3517f895cc",
   "metadata": {},
   "source": [
    "# multi head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565e1ab-d7d3-45ab-8b75-225b49989c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_self_attention(q, k, v, num_heads=2, with_scale=True, draw=True, sentences=None):\n",
    "    if sentences is None:\n",
    "        sentences = []\n",
    "\n",
    "    assert num_heads in {2, 4}\n",
    "    batch_size, query_sentence_size, embedding_size = q.shape\n",
    "    batch_size, value_sentence_size, embedding_size = v.shape\n",
    "    assert embedding_size % num_heads == 0\n",
    "\n",
    "    single_head_embedding_size = embedding_size // num_heads\n",
    "    scale = np.sqrt(single_head_embedding_size) if with_scale else 1\n",
    "\n",
    "    q = q.reshape((batch_size, query_sentence_size, num_heads, single_head_embedding_size)).transpose((0, 2, 1, 3))\n",
    "    k = k.reshape((batch_size, value_sentence_size, num_heads, single_head_embedding_size)).transpose((0, 2, 1, 3))\n",
    "    v = v.reshape((batch_size, value_sentence_size, num_heads, single_head_embedding_size)).transpose((0, 2, 1, 3))\n",
    "\n",
    "    assert q.shape == (batch_size, num_heads, query_sentence_size, single_head_embedding_size)\n",
    "    assert k.shape == (batch_size, num_heads, value_sentence_size, single_head_embedding_size)\n",
    "    assert v.shape == (batch_size, num_heads, value_sentence_size, single_head_embedding_size)\n",
    "\n",
    "    corr = softmax(q @ k.transpose((0, 1, 3, 2)) / scale)\n",
    "    assert corr.shape == (batch_size, num_heads, value_sentence_size, value_sentence_size)\n",
    "\n",
    "    if draw:\n",
    "        for i, sentence in zip(range(batch_size), sentences):\n",
    "            sentence = sentence.split()\n",
    "            if num_heads == 2:\n",
    "                _, axs = plt.subplots(nrows=1, ncols=num_heads, figsize=(16, 8))\n",
    "            if num_heads == 4:\n",
    "                _, axs = plt.subplots(nrows=2, ncols=2, figsize=(16, 16))\n",
    "\n",
    "            for ax, df in zip(axs, corr[i]):\n",
    "                df = pd.DataFrame(df, columns=sentence, index=sentence)\n",
    "                heatmap(df, sentence, sentence, ax=ax, cmap=github_cp)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    attention = corr @ v\n",
    "    assert attention.shape == (batch_size, num_heads, query_sentence_size, single_head_embedding_size)\n",
    "    return attention.transpose((0, 2, 1, 3)).reshape((batch_size, query_sentence_size, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbfc2e-7991-4952-be7b-aa338377b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = multi_head_self_attention(q, k, v, sentences=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8085596d-aee0-471c-9300-81a160bf5cdd",
   "metadata": {},
   "source": [
    "# references\n",
    "\n",
    "- [The Transformer Family Version 2.0](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "- [Tutorial 6: Transformers and Multi-Head Attention](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- [Master Positional Encoding: Part I](https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)\n",
    "- [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
    "- [Why multi-head self attention works: math, intuitions and 10+1 hidden insights](https://theaisummer.com/self-attention/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
