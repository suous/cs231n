{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef80b09-9106-4efe-83bd-84f064d5f23d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from utils import (\n",
    "    load_cifar10,\n",
    "    batch_plot,\n",
    "    seed_everything,\n",
    "    matrix_to_diagonals,\n",
    "    accuracy,\n",
    "    weights_to_images,\n",
    ")\n",
    "\n",
    "\n",
    "seed_everything()\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "benchmark = False\n",
    "search = False\n",
    "save_weights_animation = False\n",
    "run_with_cross_entropy = False\n",
    "compare_with_linear = False\n",
    "save_weights_update = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f346658c-aa85-40be-8d84-4b69cccb2105",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Softmax\n",
    "\n",
    "## Softmax Derivative\n",
    "\n",
    "$\n",
    "f(x_{i})=\\frac{e^{x_i}}{\\sum_{j}e^{x_j}} \\tag {1}\n",
    "$\n",
    "\n",
    "If $i=j$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial f(x_{i})}{\\partial x_j} &= \\frac{e^{x_i}\\cdot \\sum_{j}e^{x_j} - e^{x_i}\\cdot e^{x_j}}{\\left(\\sum_{j}e^{x_j}\\right)^2} \\tag{D1} \\\\\n",
    "    &= \\frac{e^{x_i}\\cdot(\\sum_{j}e^{x_j} - e^{x_j})}{\\left(\\sum_{j}e^{x_j}\\right)^2} \\\\\n",
    "    &= \\frac{e^{x_i}}{\\sum_{j}e^{x_j}} \\cdot \\frac{\\sum_{j}e^{x_j} - e^{x_j}}{\\sum_{j}e^{x_j}} \\\\\n",
    "    &= \\frac{e^{x_i}}{\\sum_{j}e^{x_j}} \\cdot \\left( 1 - \\frac{e^{x_j}}{\\sum_{j}e^{x_j}} \\right) \\\\\n",
    "    &= f(x_{i})\\cdot\\left(1-f(x_{j}) \\right) \\\\\n",
    "    &= f(x_{i}) - f(x_{i})\\cdot f(x_{j})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If $i\\neq j$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial f(x_{i})}{\\partial x_j} &= \\frac{0\\cdot \\sum_{j}e^{x_j} - e^{x_i}\\cdot e^{x_j}}{(\\sum_{j}e^{x_j})^2} \\tag{D2} \\\\\n",
    "    &= \\frac{-e^{x_i}\\cdot e^{x_j}}{(\\sum_{j}e^{x_j})^2} \\\\\n",
    "    &= - \\frac{e^{x_i}}{\\sum_{j}e^{x_j}} \\cdot \\frac{e^{x_j}}{\\sum_{j}e^{x_j}} \\\\\n",
    "    &= - f(x_{i})\\cdot f(x_{j})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x_{i})}{\\partial x_j}=\n",
    "\\begin{cases}\n",
    "f(x_{i}) - f(x_{i})\\cdot f(x_{j}) &i==j\\\\\n",
    "- f(x_{i})\\cdot f(x_{j}) &i\\neq j\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1b073-ac12-4fad-9580-fd0df9cc066a",
   "metadata": {},
   "source": [
    "## Cross Entropy with Softmax\n",
    "\n",
    "$$\n",
    "L(x_{i}) = - y_i \\log f(x_i) = - \\log\\left({\\frac{e^{x_i}}{\\sum_{j}e^{x_j}}}\\right)\n",
    "$$\n",
    "\n",
    "If $i=j$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(x_{i})}{\\partial x_j} &= -\\frac{1}{f(x_i)} \\cdot \\frac{\\partial f(x_{i})}{\\partial x_j} \\\\\n",
    "    &= -\\frac{1}{f(x_i)} \\cdot f(x_{i})\\cdot\\left(1-f(x_{j}) \\right) \\\\\n",
    "    &= \\left(f(x_{j}) -1 \\right) \\\\\n",
    "    &= f(x_{j}) - 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If $i\\neq j$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(x_{i})}{\\partial x_j} &= -\\frac{1}{f(x_i)} \\cdot \\frac{\\partial f(x_{i})}{\\partial x_j} \\\\\n",
    "    &= -\\frac{1}{f(x_i)} \\cdot \\left(-f(x_{i})\\cdot f(x_{j}) \\right) \\\\\n",
    "    &= f(x_{j})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324c3667-f9b5-44dc-90dc-f932adb4b135",
   "metadata": {},
   "source": [
    "## Log Softmax Derivative\n",
    "\n",
    "$\n",
    "f(x_{i})=\\log\\left({\\frac{e^{x_i}}{\\sum_{j}e^{x_j}}}\\right) \\tag {2}\n",
    "$\n",
    "\n",
    "If $i=j$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial f(x_{i})}{\\partial x_j} &= \\frac{\\sum_{j}e^{x_j}}{e^{x_i}} \\cdot \\frac{e^{x_i}\\cdot \\sum_{j}e^{x_j} - e^{x_i}\\cdot e^{x_j}}{\\left(\\sum_{j}e^{x_j}\\right)^2} \\tag{D1} \\\\\n",
    "    &= \\frac{\\sum_{j}e^{x_j}}{e^{x_i}} \\cdot \\frac{e^{x_i}\\cdot(\\sum_{j}e^{x_j} - e^{x_j})}{\\left(\\sum_{j}e^{x_j}\\right)^2} \\\\\n",
    "    &= \\frac{\\sum_{j}e^{x_j} - e^{x_j}}{\\sum_{j}e^{x_j}} \\\\\n",
    "    &= 1 - \\frac{e^{x_j}}{\\sum_{j}e^{x_j}} \\\\\n",
    "    &= 1-f(x_{j})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If $i\\neq j$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial f(x_{i})}{\\partial x_j} &= \\frac{\\sum_{j}e^{x_j}}{e^{x_i}} \\cdot \\frac{0\\cdot \\sum_{j}e^{x_j} - e^{x_i}\\cdot e^{x_j}}{(\\sum_{j}e^{x_j})^2} \\tag{D2} \\\\\n",
    "    &= \\frac{\\sum_{j}e^{x_j}}{e^{x_i}} \\cdot \\frac{-e^{x_i}\\cdot e^{x_j}}{(\\sum_{j}e^{x_j})^2} \\\\\n",
    "    &= -\\frac{e^{x_j}}{\\sum_{j}e^{x_j}} \\\\\n",
    "    &= -f(x_{j})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x_{i})}{\\partial x_j}=\n",
    "\\begin{cases}\n",
    "1 - f(x_{j}) &i==j\\\\\n",
    "-f(x_{j}) &i\\neq j\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd79cd-8532-4778-b258-99d824dc70e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    t = x - np.max(x, axis=-1, keepdims=True)\n",
    "    return t - np.log(np.sum(np.exp(t), axis=-1, keepdims=True))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def softmax_backward_1(z):\n",
    "    return np.asarray([np.diag(i) - np.outer(i, i) for i in z])\n",
    "\n",
    "\n",
    "def softmax_backward_2(z):\n",
    "    return matrix_to_diagonals(z) - np.einsum(\"bi,bj->bij\", z, z)\n",
    "\n",
    "\n",
    "def log_softmax_backward_1(z):\n",
    "    n, c = z.shape\n",
    "    return np.tile(np.identity(c), (n, 1, 1)) - np.tile(z[:, None, :], (1, c, 1))\n",
    "\n",
    "\n",
    "def log_softmax_backward_2(z):\n",
    "    n, c = z.shape\n",
    "    return np.repeat(np.eye(c)[None, :, :], n, axis=0) - np.repeat(z[:, None, :], c, axis=1)\n",
    "\n",
    "\n",
    "x = np.random.randn(4, 5)\n",
    "z = softmax(x)\n",
    "s = log_softmax(x)\n",
    "\n",
    "assert np.allclose(z.sum(axis=1), 1)\n",
    "assert np.allclose(softmax_backward_1(z), softmax_backward_2(z))\n",
    "assert np.allclose(log_softmax_backward_1(z), log_softmax_backward_2(z))\n",
    "\n",
    "if benchmark:\n",
    "    %timeit softmax_backward_1(z)\n",
    "    %timeit softmax_backward_2(z)\n",
    "\n",
    "    %timeit log_softmax_backward_1(z)\n",
    "    %timeit log_softmax_backward_2(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf89fd-89e4-49f0-9711-871c9fb62d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bz = softmax_backward_2(z)\n",
    "bs = log_softmax_backward_2(z)\n",
    "\n",
    "tx = torch.tensor(x, requires_grad=True, dtype=torch.float)\n",
    "tx.retain_grad()\n",
    "tz = torch.nn.functional.softmax(tx, dim=1)\n",
    "ts = torch.nn.functional.log_softmax(tx, dim=1)\n",
    "\n",
    "assert np.allclose(z, tz.detach().numpy())\n",
    "assert np.allclose(s, ts.detach().numpy())\n",
    "\n",
    "assert np.allclose(bz[:, 0, :], torch.autograd.grad(list(tz[:, 0]), tx, retain_graph=True)[0].numpy())\n",
    "assert np.allclose(bs[:, 0, :], torch.autograd.grad(list(ts[:, 0]), tx, retain_graph=True)[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a432c4-24cb-473f-ada6-56eb50e12976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functional import nll_loss, nll_loss_derivative\n",
    "from nn import NLLLoss\n",
    "\n",
    "reduction = \"mean\"\n",
    "\n",
    "loss = NLLLoss(reduction=reduction)\n",
    "\n",
    "# each element in target has to have 0 <= value < C\n",
    "y = np.random.randint(0, high=x.shape[1] - 1, size=x.shape[0])\n",
    "\n",
    "o = nll_loss(log_softmax(x), y, reduction=reduction)\n",
    "sx = log_softmax(x)\n",
    "lo = loss.forward(sx, y)\n",
    "bo = loss.backward(sx, y)\n",
    "\n",
    "ty = torch.tensor(y, dtype=torch.long)\n",
    "ts = torch.nn.functional.log_softmax(tx, dim=1)\n",
    "to = torch.nn.functional.nll_loss(ts, ty, reduction=reduction)\n",
    "\n",
    "assert np.allclose(o, to.detach().numpy())\n",
    "assert np.allclose(lo, to.detach().numpy())\n",
    "assert np.allclose(\n",
    "    nll_loss_derivative(x, y, reduction=reduction), torch.autograd.grad(to, ts, retain_graph=True)[0].detach().numpy()\n",
    ")\n",
    "assert np.allclose(bo, torch.autograd.grad(to, ts, retain_graph=True)[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd96147-6a92-4c16-9b6b-362ee23d8253",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Linear\n",
    "\n",
    "## Softmax with Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046684ec-1d97-4899-ad57-16fbbb241314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functional import (\n",
    "    linear,\n",
    "    log_softmax,\n",
    "    log_softmax_derivative,\n",
    "    nll_loss,\n",
    "    nll_loss_derivative,\n",
    "    softmax,\n",
    "    cross_entropy,\n",
    ")\n",
    "\n",
    "\n",
    "num_hidden = 64\n",
    "weight = np.random.normal(size=(num_hidden, x.shape[1]), scale=10)\n",
    "bias = np.random.randn(num_hidden)\n",
    "\n",
    "torch_weight = torch.tensor(weight, requires_grad=True, dtype=torch.float)\n",
    "torch_bias = torch.tensor(bias, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "torch_test_target = torch.randint_like(torch_bias, high=x.shape[1] - 1, dtype=torch.long)\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    torch.nn.functional.cross_entropy(torch_weight, torch_test_target, reduction=\"none\").detach().numpy(),\n",
    "    cross_entropy(weight, torch_test_target.detach().numpy(), reduction=\"none\", with_softmax=True),\n",
    "    decimal=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b88cb4-e591-4cf5-8661-7f66178551f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# forward\n",
    "linear_output = linear(x, weight.T, bias)\n",
    "loss = cross_entropy(linear_output, y, reduction=\"mean\", with_softmax=True)\n",
    "\n",
    "torch_linear_output = torch.nn.functional.linear(tx, torch_weight, torch_bias)\n",
    "# torch cross entropy embeded softmax\n",
    "torch_loss = torch.nn.functional.cross_entropy(torch_linear_output, ty, reduction=\"mean\")\n",
    "\n",
    "np.testing.assert_array_almost_equal(linear_output, torch_linear_output.detach().numpy(), decimal=5)\n",
    "assert np.allclose(loss, torch_loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad17c1-8aea-43d6-94ea-a41e9626e626",
   "metadata": {},
   "source": [
    "## Log Softmax with NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a41481c-bc72-4f56-b72b-bdc4efa7a87a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# forward\n",
    "linear_output = linear(x, weight.T, bias)\n",
    "log_output, softmax_output = log_softmax(linear_output, with_softmax=True)\n",
    "loss = nll_loss(log_output, y)\n",
    "# backward\n",
    "loss_grad = nll_loss_derivative(log_output, y)\n",
    "log_grad = (np.expand_dims(loss_grad, 1) @ log_softmax_derivative(softmax_output)).squeeze(axis=1)\n",
    "weight_grad = x.T @ log_grad\n",
    "bias_grad = np.sum(log_grad, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "torch_linear_output = torch.nn.functional.linear(tx, torch_weight, torch_bias)\n",
    "torch_log_output = torch.nn.functional.log_softmax(torch_linear_output, dim=1)\n",
    "torch_loss = torch.nn.functional.nll_loss(torch_log_output, ty)\n",
    "torch_loss.backward()\n",
    "\n",
    "np.testing.assert_array_almost_equal(linear_output, torch_linear_output.detach().numpy(), decimal=5)\n",
    "np.testing.assert_array_almost_equal(log_output, torch_log_output.detach().numpy(), decimal=5)\n",
    "assert np.allclose(loss, torch_loss.detach().numpy())\n",
    "\n",
    "assert np.allclose(weight_grad.T, torch_weight.grad.detach().numpy())\n",
    "assert np.allclose(bias_grad, torch_bias.grad.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f7d593-7b0a-49b6-8ad2-217c6eda9f22",
   "metadata": {},
   "source": [
    "# Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb59caf3-6187-4ecb-a4df-8c4a1d812d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_cifar10(\"../code/cs231n/datasets/cifar-10-batches-py/\")\n",
    "\n",
    "assert X_train.shape == (50000, 32, 32, 3)\n",
    "assert y_train.shape == (50000,)\n",
    "assert X_test.shape == (10000, 32, 32, 3)\n",
    "assert y_test.shape == (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea53d4-182f-44f1-9245-1c98eb67c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: airplane\n",
    "# 1. automobile\n",
    "# 2: bird\n",
    "# 3: cat\n",
    "target_classes = [0, 1, 2, 3]\n",
    "train_indices = np.isin(y_train, target_classes)\n",
    "test_indices = np.isin(y_test, target_classes)\n",
    "\n",
    "valid_rate = 0.2\n",
    "\n",
    "X_train, y_train = X_train[train_indices], y_train[train_indices]\n",
    "X_test, y_test = X_test[test_indices], y_test[test_indices]\n",
    "\n",
    "num_valid = int(len(X_train) * 0.2)\n",
    "X_valid, y_valid = X_train[-num_valid:], y_train[-num_valid:]\n",
    "X_train, y_train = X_train[:-num_valid], y_train[:-num_valid]\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_valid: {X_valid.shape}, y_valid: {y_valid.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb6eb3-57b0-4aaf-b94f-58d515757aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.bar(*np.unique(y_train, return_counts=True))\n",
    "plt.bar(*np.unique(y_valid, return_counts=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f947aa1-16d2-4205-b769-8f5ca77ad8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indexes of 'batch_size' random digits\n",
    "batch_size = 16\n",
    "random_indexes = np.random.randint(X_train.shape[0], size=batch_size)\n",
    "# Plot digits with labels\n",
    "batch_plot(X_train[random_indexes], y_train[random_indexes], with_border=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a54a9-b56a-4afb-a701-3dd1e1ffdb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train.astype(float) - mean_image).reshape((X_train.shape[0], -1))\n",
    "X_valid = (X_valid.astype(float) - mean_image).reshape((X_valid.shape[0], -1))\n",
    "X_test = (X_test.astype(float) - mean_image).reshape((X_test.shape[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39607063-2ea5-4262-8006-17ebaecc5228",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Search a better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea1ebd1-2fab-43a9-bcd3-f1df05ad5fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nn import Linear, ReLU, LogSoftmax, NLLLoss, SGD, Sequential, CrossEntropy\n",
    "\n",
    "\n",
    "# With random weights and bias with an two layer neural network\n",
    "hidden_units = 100\n",
    "num_classes = len(target_classes)\n",
    "(m, n), o = X_train.shape, y.max() + 1\n",
    "model = Sequential(\n",
    "    [\n",
    "        Linear(n, hidden_units),\n",
    "        ReLU(),\n",
    "        Linear(hidden_units, num_classes),\n",
    "        LogSoftmax(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preds = np.argmax(model.predict(X_test), axis=1)\n",
    "accuracy(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b1a6d9-e5fd-4baf-b3da-5ed41f35aed3",
   "metadata": {},
   "source": [
    "## compare with svm and softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdd216-1f19-4291-8ec1-ffbfc9b8e263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if compare_with_linear:\n",
    "    import io\n",
    "    from PIL import Image\n",
    "    from functional import relu\n",
    "\n",
    "    tmp_dir = Path(tempfile.mkdtemp())\n",
    "\n",
    "    # predict on sample images\n",
    "    sample_images = {\"cat\": 110, \"bird\": 314, \"airplane_0\": 153, \"automobile\": 49, \"airplane_1\": 25}\n",
    "    # With optimized weights and bias\n",
    "    hidden_units = len(target_classes) * 4\n",
    "\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Linear(n, hidden_units, regularization=5),\n",
    "            ReLU(),\n",
    "            Linear(hidden_units, num_classes),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    X_sample = np.concatenate([X_train, X_valid])[list(sample_images.values())]\n",
    "    preds = model.predict(X_sample)\n",
    "    preds = softmax(preds - np.max(preds, axis=-1, keepdims=True))\n",
    "    print(\"preds on epoch 0\" + \"=\" * 50)\n",
    "    print(preds)\n",
    "\n",
    "    optimizer = SGD(params=model.parameters, learning_rate=3e-4)\n",
    "    loss = CrossEntropy()\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    history = model.fit(\n",
    "        np.concatenate([X_train, X_valid]), np.concatenate([y_train, y_valid]), epochs=50, store_weights=True\n",
    "    )\n",
    "\n",
    "    train_preds = np.argmax(model.predict(X_train), axis=1)\n",
    "    valid_preds = np.argmax(model.predict(X_valid), axis=1)\n",
    "    test_preds = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "    print(f\"train acc: {accuracy(y_train, train_preds)}\")\n",
    "    print(f\"test acc: {accuracy(y_test, test_preds)}\")\n",
    "\n",
    "    X_sample = np.concatenate([X_train, X_valid])[list(sample_images.values())]\n",
    "    preds = model.predict(X_sample)\n",
    "    preds = softmax(preds - np.max(preds, axis=-1, keepdims=True))\n",
    "    print(\"preds on epoch 0\" + \"=\" * 50)\n",
    "    print(preds)\n",
    "\n",
    "    template_weights = softmax(model.parameters[2].value)\n",
    "    for epoch in [0, 49]:\n",
    "        weights = history[\"weights\"][epoch]\n",
    "        # layer 2 as weighted sum of layer 1 templates\n",
    "        weights = (relu(history[\"weights\"][epoch]) @ template_weights).T\n",
    "        weights_image = weights_to_images(weights.reshape((-1, 32, 32, 3)))\n",
    "\n",
    "        img_buf = io.BytesIO()\n",
    "        batch_plot(weights_image, with_border=False, save_path=img_buf, flatten_layout=True, flatten_columns=True)\n",
    "        Image.open(img_buf).save(tmp_dir.joinpath(f\"neural_network_weighted_weights_epoch_{epoch:04d}.png\"))\n",
    "        img_buf.close()\n",
    "\n",
    "    weights = history[\"weights\"][-1]\n",
    "    weights_image = weights_to_images(weights.T.reshape((-1, 32, 32, 3)))\n",
    "\n",
    "    img_buf = io.BytesIO()\n",
    "    batch_plot(weights_image, with_border=False, save_path=img_buf, flatten_layout=True, flatten_columns=True)\n",
    "    Image.open(img_buf).save(tmp_dir.joinpath(f\"neural_network_weights_epoch_{epoch:04d}.png\"))\n",
    "    img_buf.close()\n",
    "    # for i, img in enumerate(weights_image):\n",
    "    #     Image.fromarray(img).resize((128, 128)).save(tmp_dir.joinpath(f\"neural_network_weights_{i}_epoch_{epoch:04d}.png\"))\n",
    "    print(f\"templates weights: {template_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!open {tmp_dir}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbf597a6faf39b6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83b340-e394-49fe-b828-0bd8e56f5bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def grid_search(epoch=30, learning_rates=None, regularization_strengths=None, num_hiddens=None, verbose=True):\n",
    "    if learning_rates is None:\n",
    "        learning_rates = [1e-4, 3e-4]\n",
    "    if regularization_strengths is None:\n",
    "        regularization_strengths = np.logspace(-2, 2, 3, endpoint=True)\n",
    "    if num_hidden is None:\n",
    "        num_hiddens = np.power(2, np.arange(6, 9))\n",
    "\n",
    "    scores = {}\n",
    "    best_val_acc = -1\n",
    "\n",
    "    for lr, reg, hidden in itertools.product(learning_rates, regularization_strengths, num_hiddens):\n",
    "        model = Sequential(\n",
    "            [\n",
    "                Linear(n, hidden, regularization=reg),\n",
    "                ReLU(),\n",
    "                Linear(hidden, num_classes),\n",
    "                LogSoftmax(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        optimizer = SGD(params=model.parameters, learning_rate=lr)\n",
    "        loss = NLLLoss()\n",
    "\n",
    "        model.compile(loss=loss, optimizer=optimizer)\n",
    "        model.fit(X_train, y_train, epochs=epoch, validation_data=(X_valid, y_valid), verbose=False)\n",
    "\n",
    "        train_preds = np.argmax(model.predict(X_train), axis=1)\n",
    "        valid_preds = np.argmax(model.predict(X_valid), axis=1)\n",
    "\n",
    "        train_acc = accuracy(y_train, train_preds)\n",
    "        valid_acc = accuracy(y_valid, valid_preds)\n",
    "\n",
    "        scores[(lr, reg, hidden)] = (train_acc, valid_acc)\n",
    "        if valid_acc >= best_val_acc:\n",
    "            best_val_acc = valid_acc\n",
    "            best_model = model\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"best lr {lr:.2e} reg {reg:.2e} hiddens {hidden:3} train accuracy: {train_acc:.3f} val accuracy: {valid_acc:.3f}\"\n",
    "                )\n",
    "\n",
    "    return scores, best_model\n",
    "\n",
    "\n",
    "if search:\n",
    "    scores, best_model = grid_search(\n",
    "        learning_rates=[2e-4, 3e-4],\n",
    "        num_hiddens=[64, 81, 100, 121],\n",
    "        regularization_strengths=[5, 10, 20, 30],\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(\"=\" * 20)\n",
    "    for (lr, reg, hidden), (train_acc, valid_acc) in scores.items():\n",
    "        print(\n",
    "            f\"lr {lr:.2e} reg {reg:.2e} hiddens {hidden} train accuracy: {train_acc:.3f} val accuracy: {valid_acc:.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d845fe5-cd36-4cd0-a710-225a1101cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_with_cross_entropy:\n",
    "    # With optimized weights and bias\n",
    "    hidden_units = 121\n",
    "\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Linear(n, hidden_units, regularization=5),\n",
    "            ReLU(),\n",
    "            Linear(hidden_units, num_classes),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    optimizer = SGD(params=model.parameters, learning_rate=3e-4)\n",
    "    loss = CrossEntropy()\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    history = model.fit(\n",
    "        X_train, y_train, epochs=50, validation_data=(X_valid, y_valid), store_weights=save_weights_animation\n",
    "    )\n",
    "\n",
    "    train_preds = np.argmax(model.predict(X_train), axis=1)\n",
    "    valid_preds = np.argmax(model.predict(X_valid), axis=1)\n",
    "    test_preds = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "    print(f\"train acc: {accuracy(y_train, train_preds)}\")\n",
    "    print(f\"valid acc: {accuracy(y_valid, valid_preds)}\")\n",
    "    print(f\"test acc: {accuracy(y_test, test_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b5dc9-4bb4-42e2-a70f-01ff2ae32b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# With optimized weights and bias\n",
    "hidden_units = 121\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Linear(n, hidden_units, regularization=5),\n",
    "        ReLU(),\n",
    "        Linear(hidden_units, num_classes),\n",
    "        LogSoftmax(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = SGD(params=model.parameters, learning_rate=3e-4)\n",
    "loss = NLLLoss()\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "history = model.fit(\n",
    "    X_train, y_train, epochs=50, validation_data=(X_valid, y_valid), store_weights=save_weights_animation\n",
    ")\n",
    "\n",
    "train_preds = np.argmax(model.predict(X_train), axis=1)\n",
    "valid_preds = np.argmax(model.predict(X_valid), axis=1)\n",
    "test_preds = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "print(f\"train acc: {accuracy(y_train, train_preds)}\")\n",
    "print(f\"valid acc: {accuracy(y_valid, valid_preds)}\")\n",
    "print(f\"test acc: {accuracy(y_test, test_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d120f5-65fa-4d2a-9c46-46335104f2d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(history[\"train_loss\"])\n",
    "plt.plot(history[\"valid_loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8bcf4d-4327-4770-b95c-1fc2cadee06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linear_weights = model.parameters[0].value.T.reshape((-1, 32, 32, 3))\n",
    "linear_weights_image = weights_to_images(linear_weights)\n",
    "\n",
    "batch_plot(linear_weights_image, with_border=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a3b47-eb0a-42da-8c97-a3aa15e18eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save_weights_update:\n",
    "    import io\n",
    "    from PIL import Image\n",
    "\n",
    "    total_weights = []\n",
    "    update_steps = 2\n",
    "\n",
    "    for weights in history[\"weights\"][::update_steps]:\n",
    "        weights_image = weights_to_images(weights.T.reshape((-1, 32, 32, 3)))\n",
    "\n",
    "        img_buf = io.BytesIO()\n",
    "        batch_plot(weights_image, with_border=False, save_path=img_buf)\n",
    "        total_weights.append(Image.open(img_buf).resize((512, 512)).copy())\n",
    "        img_buf.close()\n",
    "\n",
    "    total_weights[0].save(\n",
    "        f\"images/neural_network_weights_update.gif\",\n",
    "        save_all=True,\n",
    "        append_images=total_weights[1:],\n",
    "        optimize=False,\n",
    "        duration=100,\n",
    "        loop=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83a2aa-2fd3-4d72-bca5-782c12500807",
   "metadata": {},
   "source": [
    "<img src=\"images/neural_network_weights_update.gif\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a11f74-0247-4196-8867-c32c61c46085",
   "metadata": {},
   "source": [
    "<img src=\"images/neural_network_training_epoch_0049.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
