{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import datasets as misc\n",
    "from sklearn.datasets import load_sample_images\n",
    "\n",
    "from utils import batch_plot\n",
    "from conv import generate_output_size\n",
    "\n",
    "\n",
    "benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# gray"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8d41812cbadb4e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_image = misc.ascent()\n",
    "sample_image = sample_image / sample_image.max()\n",
    "batch_plot(np.expand_dims(sample_image, 0), with_border=False, cmap=\"gray\", imgsize=6)\n",
    "\n",
    "sample_image = torch.from_numpy(sample_image)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5e44809fcc4bb5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "patch_size = 64\n",
    "image_height, image_width = sample_image.shape\n",
    "stride_height, stride_width = sample_image.stride()\n",
    "\n",
    "output_height = generate_output_size(image_height, kernel_size=patch_size, stride=patch_size, padding=0)\n",
    "output_width = generate_output_size(image_width, kernel_size=patch_size, stride=patch_size, padding=0)\n",
    "\n",
    "patches = torch.as_strided(\n",
    "    sample_image,\n",
    "    size=(output_height, output_width, patch_size, patch_size),\n",
    "    stride=(stride_height * patch_size, stride_width * patch_size, stride_height, stride_width),\n",
    ")\n",
    "\n",
    "assert torch.allclose(\n",
    "    sample_image.reshape(output_height, patch_size, output_width, patch_size).transpose(1, 2), patches\n",
    ")\n",
    "\n",
    "assert torch.allclose(sample_image.unfold(0, patch_size, patch_size).unfold(1, patch_size, patch_size), patches)\n",
    "\n",
    "if benchmark:\n",
    "    %timeit torch.as_strided(sample_image, size=(output_height, output_width, patch_size, patch_size), stride=(stride_height*patch_size, stride_width*patch_size, stride_height, stride_width))\n",
    "    %timeit sample_image.reshape(output_height, patch_size, output_width, patch_size).transpose(1,2)\n",
    "    %timeit sample_image.unfold(0, patch_size, patch_size).unfold(1, patch_size, patch_size)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e07a28c43a27bb21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_plot(\n",
    "    patches.reshape(-1, patch_size, patch_size).numpy(),\n",
    "    with_border=False,\n",
    "    cmap=\"gray\",\n",
    "    tight_layout=None,\n",
    "    wspace=0.01,\n",
    "    hspace=0.01,\n",
    "    imgsize=2,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "202fb360b9af6cb5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# color"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e31edf8dbe0fd0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_image = misc.face()\n",
    "n = np.min(sample_image.shape[:2])\n",
    "sample_image = sample_image[:n, :n, :]\n",
    "sample_image = sample_image / sample_image.max()\n",
    "batch_plot(np.expand_dims(sample_image, 0), with_border=False, cmap=\"gray\", imgsize=6)\n",
    "\n",
    "sample_image = torch.from_numpy(sample_image)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c3461a85efa5cbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "patch_size = 96\n",
    "image_height, image_width, channel = sample_image.shape\n",
    "stride_height, stride_width, stride_channel = sample_image.stride()\n",
    "\n",
    "output_height = generate_output_size(image_height, kernel_size=patch_size, stride=patch_size, padding=0)\n",
    "output_width = generate_output_size(image_width, kernel_size=patch_size, stride=patch_size, padding=0)\n",
    "\n",
    "patches = torch.as_strided(\n",
    "    sample_image,\n",
    "    size=(output_height, output_width, patch_size, patch_size, channel),\n",
    "    stride=(stride_height * patch_size, stride_width * patch_size, stride_height, stride_width, stride_channel),\n",
    ")\n",
    "\n",
    "assert torch.allclose(\n",
    "    sample_image.reshape(output_height, patch_size, output_width, patch_size, channel).transpose(1, 2), patches\n",
    ")\n",
    "\n",
    "assert torch.allclose(\n",
    "    sample_image.unfold(0, patch_size, patch_size).unfold(1, patch_size, patch_size).permute(0, 1, 3, 4, 2), patches\n",
    ")\n",
    "\n",
    "if benchmark:\n",
    "    %timeit torch.as_strided(sample_image, size=(output_height, output_width, patch_size, patch_size, channel), stride=(stride_height*patch_size, stride_width*patch_size, stride_height, stride_width, stride_channel))\n",
    "    %timeit sample_image.reshape(output_height, patch_size, output_width, patch_size, channel).transpose(1,2)\n",
    "    %timeit sample_image.unfold(0, patch_size, patch_size).unfold(1, patch_size, patch_size).permute(0,1,3,4,2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3589ad09bc53b345"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_plot(\n",
    "    patches.reshape(-1, patch_size, patch_size, channel).numpy(),\n",
    "    with_border=False,\n",
    "    cmap=\"gray\",\n",
    "    tight_layout=None,\n",
    "    wspace=0.01,\n",
    "    hspace=0.01,\n",
    "    imgsize=2,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcd97241fd488a9d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# batch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "558870240c1595db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_images = np.asarray(load_sample_images().images)\n",
    "n = 420\n",
    "sample_images = sample_images[:, :n, :n, :]\n",
    "sample_images = sample_images / sample_images.max()\n",
    "\n",
    "batch_plot(sample_images, with_border=False, imgsize=6)\n",
    "sample_images = torch.from_numpy(sample_images)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f06f986b7e7c2e72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "patch_size = 42\n",
    "batch_size, image_height, image_width, channel = sample_images.shape\n",
    "stride_batch, stride_height, stride_width, stride_channel = sample_images.stride()\n",
    "\n",
    "output_height = generate_output_size(image_height, kernel_size=patch_size, stride=patch_size, padding=0)\n",
    "output_width = generate_output_size(image_width, kernel_size=patch_size, stride=patch_size, padding=0)\n",
    "\n",
    "patches = torch.as_strided(\n",
    "    sample_images,\n",
    "    size=(batch_size, output_height, output_width, patch_size, patch_size, channel),\n",
    "    stride=(\n",
    "        stride_batch,\n",
    "        stride_height * patch_size,\n",
    "        stride_width * patch_size,\n",
    "        stride_height,\n",
    "        stride_width,\n",
    "        stride_channel,\n",
    "    ),\n",
    ")\n",
    "\n",
    "assert torch.allclose(\n",
    "    sample_images.reshape(batch_size, output_height, patch_size, output_width, patch_size, channel).transpose(2, 3),\n",
    "    patches,\n",
    ")\n",
    "\n",
    "assert torch.allclose(\n",
    "    sample_images.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size).permute(0, 1, 2, 4, 5, 3),\n",
    "    patches,\n",
    ")\n",
    "\n",
    "if benchmark:\n",
    "    %timeit torch.as_strided(sample_images, size=(batch_size, output_height, output_width, patch_size, patch_size, channel), stride=(stride_batch, stride_height*patch_size, stride_width*patch_size, stride_height, stride_width, stride_channel))\n",
    "    %timeit sample_images.reshape(batch_size, output_height, patch_size, output_width, patch_size, channel).transpose(2,3)\n",
    "    %timeit sample_images.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size).permute(0,1,2,4,5,3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bc08c8c5fc04a97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for patch in patches:\n",
    "    batch_plot(\n",
    "        patch.reshape(-1, patch_size, patch_size, channel).numpy(),\n",
    "        with_border=False,\n",
    "        cmap=\"gray\",\n",
    "        tight_layout=None,\n",
    "        wspace=0.01,\n",
    "        hspace=0.01,\n",
    "        imgsize=2,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9efde1e15d9d018"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
